{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7929592,"sourceType":"datasetVersion","datasetId":4660677},{"sourceId":7929617,"sourceType":"datasetVersion","datasetId":4660696},{"sourceId":7970025,"sourceType":"datasetVersion","datasetId":4689533,"isSourceIdPinned":true}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport rouge\nfrom transformers import AutoConfig, AutoTokenizer, MBartForConditionalGeneration\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n#from utils.preprocess import *\nfrom rouge import Rouge     \nimport wandb\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-21T15:55:35.813059Z","iopub.execute_input":"2024-04-21T15:55:35.813425Z","iopub.status.idle":"2024-04-21T15:56:08.205469Z","shell.execute_reply.started":"2024-04-21T15:55:35.813396Z","shell.execute_reply":"2024-04-21T15:56:08.204707Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"},{"name":"stderr","text":"2024-04-21 15:55:58.473719: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-21 15:55:58.473823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-21 15:55:58.593067: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('vinai/bartpho-word-base')\nmodel = MBartForConditionalGeneration.from_pretrained('vinai/bartpho-word-base')\n#tokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:56:08.206986Z","iopub.execute_input":"2024-04-21T15:56:08.207305Z","iopub.status.idle":"2024-04-21T15:56:13.827227Z","shell.execute_reply.started":"2024-04-21T15:56:08.207243Z","shell.execute_reply":"2024-04-21T15:56:13.826454Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/898 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d485be2a7ed47259abae1d0f3502b51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1e235c470c0474bb6e90ee22c6524ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b45578eb519412cb2bdda1b78d221b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5491f894550441318da965ff0f8d8f34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/600M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e26030408b6148668fd87ae017789c6c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_rouge1_fmeasure_score_vlsp(preds, target):\n    rouge = Rouge()\n    scores = rouge.get_scores(preds, target, avg=False)\n    rouge1_fmeasure= scores[0]['rouge-1'][\"f\"]\n    rouge2_fmeasure= scores[0]['rouge-2'][\"f\"]\n    rougeL_fmeasure= scores[0]['rouge-l'][\"f\"]\n    return {\n        'rouge_1': rouge1_fmeasure,\n        'rouge_2': rouge2_fmeasure,\n        'rouge_L': rougeL_fmeasure,\n    }\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = compute_rouge1_fmeasure_score_vlsp(preds=decoded_preds, target=decoded_labels)\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result['gen_len'] = np.mean(prediction_lens)\n\n    return {\n        'rouge-1': result['rouge_1'],\n        'rouge_2': result['rouge_2'],\n        'rouge_L': result['rouge_L'],\n        'gen_len': result['gen_len'],\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:56:13.828444Z","iopub.execute_input":"2024-04-21T15:56:13.828732Z","iopub.status.idle":"2024-04-21T15:56:13.837001Z","shell.execute_reply.started":"2024-04-21T15:56:13.828707Z","shell.execute_reply":"2024-04-21T15:56:13.836080Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(sample):\n    # Tạo input cho mô hình từ câu hỏi và nội dung\n    inputs = sample[\"question\"] + \" \" + sample[\"context\"]\n    # Tạo nhãn cho mô hình từ câu trả lời\n    #labels = sample[\"answer\"]\n\n\n    # tokenize inputs\n    #model_inputs = tokenizer(inputs, max_length=1024, padding='max_length', truncation=True)\n    model_inputs = tokenizer(\n        sample[\"question\"],\n        sample[\"context\"],\n        max_length=1024,\n        truncation= \"only_second\",\n        stride=50,\n        padding = True\n        #return_overflowing_tokens=True,\n        #return_offsets_mapping=True,\n        )\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=sample[\"answer\"], max_length=1024, padding='max_length', truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n \n    return {\n        'input_ids': model_inputs['input_ids'],\n        'attention_mask': model_inputs['attention_mask'],\n        'labels': model_inputs['labels']\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:56:13.839669Z","iopub.execute_input":"2024-04-21T15:56:13.840003Z","iopub.status.idle":"2024-04-21T15:56:13.850074Z","shell.execute_reply.started":"2024-04-21T15:56:13.839972Z","shell.execute_reply":"2024-04-21T15:56:13.849312Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nimport json\ndef merge_json_files(file1_path, file2_path):\n    # Đọc dữ liệu từ file 1\n    with open(file1_path, 'r', encoding='utf-8') as file1:\n        data1 = json.load(file1)\n    \n    # Đọc dữ liệu từ file 2\n    with open(file2_path, 'r', encoding='utf-8') as file2:\n        data2 = json.load(file2)\n    \n    merged_data = []\n    x = len(data2)//len(data1)\n    # Nối dữ liệu từ file 1 và file 2, chỉ giữ lại các trường \"question\", \"context\", và \"answer\"\n    i, j = 0, 0\n    while i < len(data1) and j < len(data2):\n        for _ in range(x):\n            if j < len(data2):\n                merged_data.append(data2[j])\n                j += 1\n        merged_data.append(data1[i])\n        i += 1\n\n    # Nếu còn dữ liệu ở file 2, thêm vào cuối\n    while j < len(data2):\n        merged_data.append(data2[j])\n        j += 1\n\n    return merged_data","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:56:13.851042Z","iopub.execute_input":"2024-04-21T15:56:13.851320Z","iopub.status.idle":"2024-04-21T15:56:13.864197Z","shell.execute_reply.started":"2024-04-21T15:56:13.851296Z","shell.execute_reply":"2024-04-21T15:56:13.863397Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def load_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:56:13.865213Z","iopub.execute_input":"2024-04-21T15:56:13.865562Z","iopub.status.idle":"2024-04-21T15:56:13.874088Z","shell.execute_reply.started":"2024-04-21T15:56:13.865538Z","shell.execute_reply":"2024-04-21T15:56:13.873074Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Đọc dữ liệu từ file JSON\ntrain_data = load_json_file('/kaggle/input/law-qa/Law-QA-train.json')\ntest_data = load_json_file('/kaggle/input/law-qa/Law-QA-test.json')\n\ncolumns = [\"question\", \"context\", \"answer\"]\n\n# Tạo danh sách dữ liệu của các cột\nlist_data_train = {column: [item[column] for item in train_data] for column in columns}\nlist_data_test = {column: [item[column] for item in test_data] for column in columns}\n# Tạo đối tượng Dataset từ danh sách dữ liệu\ntrain = Dataset.from_dict(list_data_train)\ntest = Dataset.from_dict(list_data_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:56:13.875156Z","iopub.execute_input":"2024-04-21T15:56:13.875481Z","iopub.status.idle":"2024-04-21T15:56:33.131891Z","shell.execute_reply.started":"2024-04-21T15:56:13.875458Z","shell.execute_reply":"2024-04-21T15:56:33.131089Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train_test_splits():\n    train_data = (train.map(preprocess_function, remove_columns=['question', 'context', 'answer']))\n    test_data = (test.map(preprocess_function, remove_columns=['question', 'context', 'answer']))\n    return train_data, test_data\n\nprint('------------------------------------------------Loading Data----------------------------------------------------')\ntrain_data, test_data = train_test_splits()\nprint('----------------------------------------------------Done!-------------------------------------------------------')","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:56:33.133255Z","iopub.execute_input":"2024-04-21T15:56:33.134096Z","iopub.status.idle":"2024-04-21T16:11:37.849028Z","shell.execute_reply.started":"2024-04-21T15:56:33.134062Z","shell.execute_reply":"2024-04-21T16:11:37.848076Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"------------------------------------------------Loading Data----------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/168000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce048c67fb584e5aa32d3fad23c3d77a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2898 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed4e2eda1504d0697acd2800be604d1"}},"metadata":{}},{"name":"stdout","text":"----------------------------------------------------Done!-------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Subset\nsubset_size = 144000\nsuset_size = 200\nsubset_train_data = Subset(train_data, range(len(train_data) - subset_size, len(train_data)))\nsubset_valid_data = Subset(test_data, range(suset_size))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T16:11:37.850216Z","iopub.execute_input":"2024-04-21T16:11:37.850529Z","iopub.status.idle":"2024-04-21T16:11:37.855442Z","shell.execute_reply.started":"2024-04-21T16:11:37.850504Z","shell.execute_reply":"2024-04-21T16:11:37.854633Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    pad_to_multiple_of=8,\n    #padding = True\n)\n\ntraining_args =Seq2SeqTrainingArguments(\n    output_dir=\"./Checkpoint Bartpho\",\n    gradient_checkpointing=True,\n    do_train=True,\n    remove_unused_columns=False,\n    warmup_ratio=0.05,\n    weight_decay=0.01,\n    group_by_length=True,\n    fp16=True,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    predict_with_generate=True,\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    logging_strategy=\"steps\",\n    logging_steps=3000,\n    save_steps=3000,\n    eval_steps=3000,\n    evaluation_strategy=\"steps\",\n    save_total_limit=5,\n)\n\n\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    eval_dataset=test_data,\n    train_dataset=subset_train_data,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# tokenizer.save_pretrained('./Checkpoint Bartpho/Final/Tokenizer-Pipeline')\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T16:11:37.857872Z","iopub.execute_input":"2024-04-21T16:11:37.858132Z","iopub.status.idle":"2024-04-22T02:53:27.269452Z","shell.execute_reply.started":"2024-04-21T16:11:37.858111Z","shell.execute_reply":"2024-04-22T02:53:27.268502Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240421_163754-iqxvt6p4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nguyenbatuan/huggingface/runs/iqxvt6p4' target=\"_blank\">clean-dew-34</a></strong> to <a href='https://wandb.ai/nguyenbatuan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nguyenbatuan/huggingface' target=\"_blank\">https://wandb.ai/nguyenbatuan/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nguyenbatuan/huggingface/runs/iqxvt6p4' target=\"_blank\">https://wandb.ai/nguyenbatuan/huggingface/runs/iqxvt6p4</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9000/9000 10:14:55, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge-1</th>\n      <th>Rouge 2</th>\n      <th>Rouge L</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>3000</td>\n      <td>0.370700</td>\n      <td>0.116160</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.873706</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.120500</td>\n      <td>0.100153</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.891994</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.108900</td>\n      <td>0.093353</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.885438</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9000, training_loss=0.2000576477050781, metrics={'train_runtime': 38259.4018, 'train_samples_per_second': 3.764, 'train_steps_per_second': 0.235, 'total_flos': 6.932390312293171e+16, 'train_loss': 0.2000576477050781, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"#trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T02:53:27.270786Z","iopub.execute_input":"2024-04-22T02:53:27.271109Z","iopub.status.idle":"2024-04-22T02:53:27.276876Z","shell.execute_reply.started":"2024-04-22T02:53:27.271085Z","shell.execute_reply":"2024-04-22T02:53:27.275716Z"},"trusted":true},"execution_count":11,"outputs":[]}]}