{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7859982,"sourceType":"datasetVersion","datasetId":4610612},{"sourceId":7860148,"sourceType":"datasetVersion","datasetId":4610742},{"sourceId":7885868,"sourceType":"datasetVersion","datasetId":4629197}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport rouge\nfrom transformers import AutoConfig, AutoTokenizer, MBartForConditionalGeneration\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n#from utils.preprocess import *\nfrom rouge import Rouge     \nimport wandb\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-03T01:37:30.160480Z","iopub.execute_input":"2024-04-03T01:37:30.161028Z","iopub.status.idle":"2024-04-03T01:38:01.835511Z","shell.execute_reply.started":"2024-04-03T01:37:30.161002Z","shell.execute_reply":"2024-04-03T01:38:01.834638Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"},{"name":"stderr","text":"2024-04-03 01:37:52.373125: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-03 01:37:52.373221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-03 01:37:52.498239: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('vinai/bartpho-word-base')\nmodel = MBartForConditionalGeneration.from_pretrained('vinai/bartpho-word-base')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:38:01.837004Z","iopub.execute_input":"2024-04-03T01:38:01.837562Z","iopub.status.idle":"2024-04-03T01:38:44.428805Z","shell.execute_reply.started":"2024-04-03T01:38:01.837532Z","shell.execute_reply":"2024-04-03T01:38:44.427975Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/898 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31d3b8b2ac6c4f8db9abae73c8a76f2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f1505485734173ae75df4008a17a14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ed24121ec5c4a159ff888031e84a5a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a7c7590e084db19aafffa0a9254e9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/600M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8457e3aedd04d518c9546a4e685cd67"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_rouge1_fmeasure_score_vlsp(preds, target):\n    rouge = Rouge()\n    scores = rouge.get_scores(preds, target, avg=False)\n    rouge1_fmeasure= scores[0]['rouge-1'][\"f\"]\n    rouge2_fmeasure= scores[0]['rouge-2'][\"f\"]\n    rougeL_fmeasure= scores[0]['rouge-l'][\"f\"]\n    return {\n        'rouge_1': rouge1_fmeasure,\n        'rouge_2': rouge2_fmeasure,\n        'rouge_L': rougeL_fmeasure,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:38:44.430023Z","iopub.execute_input":"2024-04-03T01:38:44.430304Z","iopub.status.idle":"2024-04-03T01:38:44.436129Z","shell.execute_reply.started":"2024-04-03T01:38:44.430279Z","shell.execute_reply":"2024-04-03T01:38:44.435150Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(sample):\n    # Tạo input cho mô hình từ câu hỏi và nội dung\n    inputs = sample[\"question\"] + \" \" + sample[\"context\"]\n    # Tạo nhãn cho mô hình từ câu trả lời\n    #labels = sample[\"answer\"]\n\n\n    # tokenize inputs\n    #model_inputs = tokenizer(inputs, max_length=1024, padding='max_length', truncation=True)\n    model_inputs = tokenizer(\n        sample[\"question\"],\n        sample[\"context\"],\n        max_length=1024,\n        truncation= \"only_second\",\n        stride=50,\n        padding = True\n        #return_overflowing_tokens=True,\n        #return_offsets_mapping=True,\n        )\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=sample[\"answer\"], max_length=1024, padding='max_length', truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n \n    return {\n        'input_ids': model_inputs['input_ids'],\n        'attention_mask': model_inputs['attention_mask'],\n        'labels': model_inputs['labels']\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:38:44.438467Z","iopub.execute_input":"2024-04-03T01:38:44.438747Z","iopub.status.idle":"2024-04-03T01:38:44.455197Z","shell.execute_reply.started":"2024-04-03T01:38:44.438723Z","shell.execute_reply":"2024-04-03T01:38:44.454364Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = compute_rouge1_fmeasure_score_vlsp(preds=decoded_preds, target=decoded_labels)\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result['gen_len'] = np.mean(prediction_lens)\n\n    return {\n        'rouge-1': result['rouge_1'],\n        'rouge_2': result['rouge_2'],\n        'rouge_L': result['rouge_L'],\n        'gen_len': result['gen_len'],\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:38:44.456249Z","iopub.execute_input":"2024-04-03T01:38:44.456563Z","iopub.status.idle":"2024-04-03T01:38:44.465353Z","shell.execute_reply.started":"2024-04-03T01:38:44.456533Z","shell.execute_reply":"2024-04-03T01:38:44.464468Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nimport json\ndef merge_json_files(file1_path, file2_path):\n    # Đọc dữ liệu từ file 1\n    with open(file1_path, 'r', encoding='utf-8') as file1:\n        data1 = json.load(file1)\n    \n    # Đọc dữ liệu từ file 2\n    with open(file2_path, 'r', encoding='utf-8') as file2:\n        data2 = json.load(file2)\n    \n    merged_data = []\n    x = len(data2)//len(data1)\n    # Nối dữ liệu từ file 1 và file 2, chỉ giữ lại các trường \"question\", \"context\", và \"answer\"\n    i, j = 0, 0\n    while i < len(data1) and j < len(data2):\n        merged_data.append(data1[i])\n        for _ in range(x):\n            if j < len(data2):\n                merged_data.append(data2[j])\n                j += 1\n        i += 1\n\n    # Nếu còn dữ liệu ở file 2, thêm vào cuối\n    while j < len(data2):\n        merged_data.append(data2[j])\n        j += 1\n\n    return merged_data","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:43:22.659298Z","iopub.execute_input":"2024-04-03T01:43:22.659974Z","iopub.status.idle":"2024-04-03T01:43:22.697919Z","shell.execute_reply.started":"2024-04-03T01:43:22.659942Z","shell.execute_reply":"2024-04-03T01:43:22.696464Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def load_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:38:44.479492Z","iopub.execute_input":"2024-04-03T01:38:44.479877Z","iopub.status.idle":"2024-04-03T01:38:44.488350Z","shell.execute_reply.started":"2024-04-03T01:38:44.479846Z","shell.execute_reply":"2024-04-03T01:38:44.487477Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\n# Đọc dữ liệu từ file JSON\ntrain_data = merge_json_files('/kaggle/input/vimqa-dataset/train_vimqa.json', '/kaggle/input/law-qa/Law-QA-train.json')\ntest_data = load_json_file('/kaggle/input/law-qa/Law-QA-test.json')\n\ncolumns = [\"question\", \"context\", \"answer\"]\n\n# Tạo danh sách dữ liệu của các cột\nlist_data_train = {column: [item[column] for item in train_data] for column in columns}\nlist_data_test = {column: [item[column] for item in test_data] for column in columns}\n# Tạo đối tượng Dataset từ danh sách dữ liệu\ntrain = Dataset.from_dict(list_data_train)\ntest = Dataset.from_dict(list_data_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:43:25.579559Z","iopub.execute_input":"2024-04-03T01:43:25.580237Z","iopub.status.idle":"2024-04-03T01:43:38.615195Z","shell.execute_reply.started":"2024-04-03T01:43:25.580205Z","shell.execute_reply":"2024-04-03T01:43:38.614389Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train_test_splits():\n    train_data = (train.map(preprocess_function, remove_columns=['question', 'context', 'answer']))\n    test_data = (test.map(preprocess_function, remove_columns=['question', 'context', 'answer']))\n    return train_data, test_data\n\nprint('------------------------------------------------Loading Data----------------------------------------------------')\ntrain_data, test_data = train_test_splits()\nprint('----------------------------------------------------Done!-------------------------------------------------------')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:44:03.202165Z","iopub.execute_input":"2024-04-03T01:44:03.203054Z","iopub.status.idle":"2024-04-03T01:59:15.394279Z","shell.execute_reply.started":"2024-04-03T01:44:03.203018Z","shell.execute_reply":"2024-04-03T01:59:15.393234Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"------------------------------------------------Loading Data----------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/176041 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c19009b13914e1e88ce26fabfa8d920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2898 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00d1f3191c1948ed888542d9c056e377"}},"metadata":{}},{"name":"stdout","text":"----------------------------------------------------Done!-------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"train[42]","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:43:55.673184Z","iopub.execute_input":"2024-04-03T01:43:55.674013Z","iopub.status.idle":"2024-04-03T01:43:55.680388Z","shell.execute_reply.started":"2024-04-03T01:43:55.673980Z","shell.execute_reply":"2024-04-03T01:43:55.679420Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'question': 'Bảng xếp hạng mà phiên bản \"I Will Always Love You\" của Parton đạt vị trí số một liệt kê danh sách 60 ca khúc phải không?',\n 'context': 'Phiên bản \"I Will Always Love You\" của Parton đã gặt hái nhiều thành công thương mại, đạt vị trí số một trên bảng xếp hạng Billboard Hot Country Songs ở hai thời điểm khác nhau. Bảng xếp hạng này liệt kê danh sách 60 bài hát thể loại nhạc đồng quê phổ biến nhất tại Hoa Kỳ, dựa trên tần suất phát thanh và doanh số đĩa.',\n 'answer': 'đúng'}"},"metadata":{}}]},{"cell_type":"code","source":"print(train_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:43:57.926267Z","iopub.execute_input":"2024-04-03T01:43:57.927110Z","iopub.status.idle":"2024-04-03T01:43:57.931609Z","shell.execute_reply.started":"2024-04-03T01:43:57.927075Z","shell.execute_reply":"2024-04-03T01:43:57.930649Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"{'question': 'Film điện ảnh mà Margalit Ruth \"Maggie\" Gyllenhaal nhận được vai phụ độc lập vào năm 2001 có kinh phí là 4,5 triệu USD phải không?', 'context': 'Cô khởi đầu sự nghiệp điện ảnh với các bộ phim được đạo diễn bởi cha mình, Stephen Gyllenhaal, và nhận được vai phụ trong phim điện ảnh độc lập Donnie Darko (2001), bộ phim với em trai cô Jake Gyllenhaal đóng vai chính. Kinh phí làm phim này là 4,5 triệu USD và được quay trong vòng 28 ngày.', 'answer': 'đúng'}\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Subset\nsubset_size = 144000\n#suset_size = 20\nsubset_train_data = Subset(train_data, range(subset_size))\n#subset_valid_dataset = Subset(valid_dataset, range(suset_size))","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:00:14.998644Z","iopub.execute_input":"2024-04-03T02:00:14.999789Z","iopub.status.idle":"2024-04-03T02:00:15.007121Z","shell.execute_reply.started":"2024-04-03T02:00:14.999744Z","shell.execute_reply":"2024-04-03T02:00:15.005755Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    pad_to_multiple_of=8,\n    #padding = True\n)\n\ntraining_args =Seq2SeqTrainingArguments(\n    output_dir=\"./Checkpoint Bartpho\",\n    gradient_checkpointing=True,\n    do_train=True,\n    remove_unused_columns=False,\n    warmup_ratio=0.05,\n    weight_decay=0.01,\n    group_by_length=True,\n    fp16=True,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    predict_with_generate=True,\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    logging_strategy=\"steps\",\n    logging_steps=1000,\n    save_steps=1000,\n    eval_steps=1000,\n    evaluation_strategy=\"steps\",\n    save_total_limit=5,\n)\n\n\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    eval_dataset=test_data,\n    train_dataset=subset_train_data,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# tokenizer.save_pretrained('./Checkpoint Bartpho/Final/Tokenizer-Pipeline')\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T02:00:17.429537Z","iopub.execute_input":"2024-04-03T02:00:17.430632Z","iopub.status.idle":"2024-04-03T12:54:53.191864Z","shell.execute_reply.started":"2024-04-03T02:00:17.430564Z","shell.execute_reply":"2024-04-03T12:54:53.190812Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240403_020528-0c79cud5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nguyenbatuan/huggingface/runs/0c79cud5' target=\"_blank\">robust-river-22</a></strong> to <a href='https://wandb.ai/nguyenbatuan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nguyenbatuan/huggingface' target=\"_blank\">https://wandb.ai/nguyenbatuan/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nguyenbatuan/huggingface/runs/0c79cud5' target=\"_blank\">https://wandb.ai/nguyenbatuan/huggingface/runs/0c79cud5</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9000/9000 10:48:47, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge-1</th>\n      <th>Rouge 2</th>\n      <th>Rouge L</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.831100</td>\n      <td>0.144306</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.847481</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.144000</td>\n      <td>0.127161</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.876121</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.130100</td>\n      <td>0.115496</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.851277</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.122300</td>\n      <td>0.109289</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.869565</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.114400</td>\n      <td>0.105548</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.909248</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.110400</td>\n      <td>0.100977</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.895445</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.108200</td>\n      <td>0.097976</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.899931</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.103400</td>\n      <td>0.095291</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.898206</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.103600</td>\n      <td>0.094279</td>\n      <td>0.333333</td>\n      <td>0.121951</td>\n      <td>0.166667</td>\n      <td>19.895100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9000, training_loss=0.19638556755913628, metrics={'train_runtime': 39028.6223, 'train_samples_per_second': 3.69, 'train_steps_per_second': 0.231, 'total_flos': 6.645281297478451e+16, 'train_loss': 0.19638556755913628, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:39:01.544517Z","iopub.status.idle":"2024-04-03T01:39:01.544943Z","shell.execute_reply.started":"2024-04-03T01:39:01.544734Z","shell.execute_reply":"2024-04-03T01:39:01.544751Z"},"trusted":true},"execution_count":null,"outputs":[]}]}